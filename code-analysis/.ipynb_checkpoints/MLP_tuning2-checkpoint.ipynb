{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "deA41dr5D9wq"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21022,
     "status": "ok",
     "timestamp": 1576475182969,
     "user": {
      "displayName": "Takeshi Koshizuka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCcRFcsMRDhF1wqEYEQ8zcMJDNe33BRGt73NxFcKQ=s64",
      "userId": "06262162256211987803"
     },
     "user_tz": -540
    },
    "id": "xFT54ROCEBcf",
    "outputId": "9089abc7-3924-4a78-c997-1e9bca1044cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hyperdash\n",
      "  Downloading https://files.pythonhosted.org/packages/fb/a1/2606aa8a8c3bf083cb305dba3164cb00285f97db34080d63c22b6c413175/hyperdash-0.15.3.tar.gz\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from hyperdash) (2.21.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from hyperdash) (1.12.0)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from hyperdash) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->hyperdash) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->hyperdash) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->hyperdash) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->hyperdash) (3.0.4)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->hyperdash) (1.3)\n",
      "Building wheels for collected packages: hyperdash\n",
      "  Building wheel for hyperdash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for hyperdash: filename=hyperdash-0.15.3-cp36-none-any.whl size=28553 sha256=9ad04ccc7dedf8e0be224f17df072f9081c50a7d51a8f27c2b4f4852d693bc5a\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/5f/af/bbcaeb6570e4904c14fb4c1b70fee559a3788182ce4d104ce7\n",
      "Successfully built hyperdash\n",
      "Installing collected packages: hyperdash\n",
      "Successfully installed hyperdash-0.15.3\n",
      "Opening browser, please wait. If something goes wrong, press CTRL+C to cancel.\n",
      "\u001b[1m SSH'd into a remote machine, or just don't have access to a browser? Open this link in any browser and then copy/paste the provided access token: \u001b[4mhttps://hyperdash.io/oauth/github/start?state=client_cli_manual\u001b[0m \u001b[0m\n",
      "Waiting for Github OAuth to complete.\n",
      "If something goes wrong, press CTRL+C to cancel.\n",
      "Access token: nCTFZUSf3c/WdqZzokhzr5WyWnmIDWEFt6iALgdoNLs=\n",
      "Successfully logged in! We also installed: yWgexKOerZf5n/R8HHG19FhvDS6ZndyExaBZ68fhmxM= as your default API key\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperdash\n",
    "from hyperdash import monitor_cell\n",
    "!hyperdash signup --github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2168,
     "status": "ok",
     "timestamp": 1576475187934,
     "user": {
      "displayName": "Takeshi Koshizuka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCcRFcsMRDhF1wqEYEQ8zcMJDNe33BRGt73NxFcKQ=s64",
      "userId": "06262162256211987803"
     },
     "user_tz": -540
    },
    "id": "0TZIpqqcEDhd",
    "outputId": "766c210a-f4ad-41ca-c664-758b5bcc3300"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "from hyperdash import Experiment\n",
    "\n",
    "class Hyperdash(Callback):\n",
    "    def __init__(self, entries, exp):\n",
    "        super(Hyperdash, self).__init__()\n",
    "        self.entries = entries\n",
    "        self.exp = exp\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for entry in self.entries:\n",
    "            log = logs.get(entry)            \n",
    "            if log is not None:\n",
    "                self.exp.metric(entry, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZDv5xnvFEFK4"
   },
   "outputs": [],
   "source": [
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30320,
     "status": "ok",
     "timestamp": 1576475217104,
     "user": {
      "displayName": "Takeshi Koshizuka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCcRFcsMRDhF1wqEYEQ8zcMJDNe33BRGt73NxFcKQ=s64",
      "userId": "06262162256211987803"
     },
     "user_tz": -540
    },
    "id": "MWqJcGN3EJU8",
    "outputId": "1bc51b0b-d98f-42fc-9caa-04d5628dc8f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31198,
     "status": "ok",
     "timestamp": 1576475218725,
     "user": {
      "displayName": "Takeshi Koshizuka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCcRFcsMRDhF1wqEYEQ8zcMJDNe33BRGt73NxFcKQ=s64",
      "userId": "06262162256211987803"
     },
     "user_tz": -540
    },
    "id": "Uh4lVEfY3pJN",
    "outputId": "f8e0232e-0133-41e6-df0f-4ff13cec7636"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 01.model\n",
      " amidaTest2.py\n",
      " bert-master.zip\n",
      " bert_x.npy\n",
      " bow_train_x.npz\n",
      " cabocha-0.69.tar.bz2\n",
      "'Colab Notebooks'\n",
      " C言語課題.gsite\n",
      " doc2vec-dmpv_x.npy\n",
      " doc2vec.py\n",
      " doc2vec_x.npy\n",
      " export\n",
      " fasttext-IMDB.bin\n",
      " fasttext-neologd.bin\n",
      " fasttext-normal.bin\n",
      " fasttext_x_hier.npy\n",
      " fasttext_x_max.npy\n",
      " fasttext_x_mean.npy\n",
      " Japanese_L-12_H-768_A-12_E-30_BPE_transformers.zip\n",
      " ja.zip\n",
      " LBa.csv\n",
      " LBa.gsheet\n",
      " modified.pdf\n",
      " n-gram-tf-idf_x.npz\n",
      " n-gram_x.npz\n",
      " nmf.pdf\n",
      " Saitaihou.pdf\n",
      " sdv.npy\n",
      " tf-idf_x.npz\n",
      " train-val_pre.tsv\n",
      " train-val-pre.tsv\n",
      " train-val-small.tsv\n",
      " train-val-wakati-juman2.tsv\n",
      " train-val-wakati-juman-nva.tsv\n",
      " train-val-wakati-juman.tsv\n",
      " train-val-wakati-nva.tsv\n",
      " train-val-wakati.tsv\n",
      " vector_data\n",
      " word2vec_x_mean.npy\n",
      " y_full.npy\n",
      " 六義園.gdoc\n",
      " 無題のプレゼンテーション.gslides\n",
      " 試作\n"
     ]
    }
   ],
   "source": [
    "!ls ./drive/My\\ Drive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4xzqCjjvELSk"
   },
   "outputs": [],
   "source": [
    "%cp ./drive/My\\ Drive/mfcc_speech_mean_max_min.npz ./\n",
    "%cp ./drive/My\\ Drive/delta_speech_mean_max_min.npz ./\n",
    "%cp ./drive/My\\ Drive/power_speech_mean_max_min.npz ./\n",
    "%cp ./drive/My\\ Drive/label_speech_origin.npz\t ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CIzVeTl4ELPy"
   },
   "outputs": [],
   "source": [
    " %mkdir model\n",
    "%mkdir model/tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36678,
     "status": "ok",
     "timestamp": 1576475226098,
     "user": {
      "displayName": "Takeshi Koshizuka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCcRFcsMRDhF1wqEYEQ8zcMJDNe33BRGt73NxFcKQ=s64",
      "userId": "06262162256211987803"
     },
     "user_tz": -540
    },
    "id": "lFXPOIxcETAj",
    "outputId": "371df40e-3f7c-4c31-ac70-f265fe4b6f94"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    " import os,sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "from sklearn.model_selection import learning_curve\n",
    "from scipy import sparse\n",
    "from scipy.sparse import load_npz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from scipy.sparse import load_npz\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "from tqdm import tqdm\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import SpatialDropout1D, Bidirectional, GlobalMaxPool1D\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.advanced_activations import ReLU, PReLU\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "class Util:\n",
    "    @classmethod\n",
    "    def dump(cls, value, path):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        joblib.dump(value, path, compress=True)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        return joblib.load(path)\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self):\n",
    "        self.general_logger = logging.getLogger('general')\n",
    "        self.result_logger = logging.getLogger('result')\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        file_general_handler = logging.FileHandler('./model/general.log')\n",
    "        file_result_handler = logging.FileHandler('./model/result.log')\n",
    "        if len(self.general_logger.handlers) == 0:\n",
    "            self.general_logger.addHandler(stream_handler)\n",
    "            self.general_logger.addHandler(file_general_handler)\n",
    "            self.general_logger.setLevel(logging.INFO)\n",
    "            self.result_logger.addHandler(stream_handler)\n",
    "            self.result_logger.addHandler(file_result_handler)\n",
    "            self.result_logger.setLevel(logging.INFO)\n",
    "\n",
    "    def info(self, message):\n",
    "        # 時刻をつけてコンソールとログに出力\n",
    "        self.general_logger.info('[{}] - {}'.format(self.now_string(), message))\n",
    "\n",
    "    def result(self, message):\n",
    "        self.result_logger.info(message)\n",
    "\n",
    "    def result_ltsv(self, dic):\n",
    "        self.result(self.to_ltsv(dic))\n",
    "\n",
    "    def result_scores(self, run_name, scores):\n",
    "        # 計算結果をコンソールと計算結果用ログに出力\n",
    "        dic = dict()\n",
    "        dic['name'] = run_name\n",
    "        dic['score'] = np.mean(scores)\n",
    "        for i, score in enumerate(scores):\n",
    "            dic[f'score{i}'] = score\n",
    "        self.result_ltsv(dic)\n",
    "\n",
    "    def now_string(self):\n",
    "        return str(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    def to_ltsv(self, dic):\n",
    "        return '\\t'.join(['{}:{}'.format(key, value) for key, value in dic.items()])\n",
    "\n",
    "class Model(metaclass=ABCMeta):\n",
    "    def __init__(self, run_fold_name: str, params: dict) -> None:\n",
    "        \"\"\"コンストラクタ\n",
    "        :param run_fold_name: ランの名前とfoldの番号を組み合わせた名前\n",
    "        :param params: ハイパーパラメータ\n",
    "        \"\"\"\n",
    "        self.run_fold_name = run_fold_name\n",
    "        self.params = params\n",
    "        self.model = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, tr_x: np.array, tr_y: np.array,\n",
    "              va_x: Optional[np.array] = None,\n",
    "              va_y: Optional[np.array] = None) -> None:\n",
    "        \"\"\"モデルの学習を行い、学習済のモデルを保存する\n",
    "        :param tr_x: 学習データの特徴量\n",
    "        :param tr_y: 学習データの目的変数\n",
    "        :param va_x: バリデーションデータの特徴量\n",
    "        :param va_y: バリデーションデータの目的変数\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, te_x: np.array) -> np.array:\n",
    "        \"\"\"学習済のモデルでの予測値を返す\n",
    "        :param te_x: バリデーションデータやテストデータの特徴量\n",
    "        :return: 予測値\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def score(self, te_x: np.array, te_y: np.array) -> float:\n",
    "        \"\"\"学習済のモデルでのスコア値を返す\n",
    "        :te_x: np.array\n",
    "        :te_y: np.array\n",
    "        :return: 予測値\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model(self, feature: str) -> None:\n",
    "        \"\"\"モデルの保存を行う\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model(self, feature: str) -> None:\n",
    "        \"\"\"モデルの読み込みを行う\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "logger = Logger()\n",
    "\n",
    "def load_x_train(features, sparse=False):\n",
    "    x = np.empty((1440, 0))\n",
    "    if \"mfcc\" in features:\n",
    "        #matrix = np.load('../data/mfcc_speech_origin.npz')['arr_0']\n",
    "        matrix = np.load('../data/mfcc_speech_mean_max_min.npz')['arr_0']\n",
    "        x = np.hstack(( x, matrix))\n",
    "        print(x.shape)\n",
    "    if \"delta\" in features:\n",
    "        #matrix = np.load('../data/delta_speech_origin.npz')['arr_0']\n",
    "        matrix = np.load('../data/delta_speech_mean_max_min.npz')['arr_0']\n",
    "        x = np.hstack(( x, matrix))\n",
    "        print(x.shape)\n",
    "    if \"power\" in features:\n",
    "        #matrix = np.load('../data/power_speech_origin.npz')['arr_0']\n",
    "        matrix = np.load('../data/power_speech_mean_max_min.npz')['arr_0']\n",
    "        x = np.hstack(( x, matrix))\n",
    "        print(x.shape)\n",
    "    train_x = x          \n",
    "    return train_x\n",
    "\n",
    "def load_y_train():\n",
    "    return np.load('../data/label_speech_origin.npz')['arr_0'].astype('int') -1\n",
    "\n",
    "\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "# tensorflowの警告抑制\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "class ModelMLP(Model):\n",
    "    def __init__(self, run_fold_name, **params):\n",
    "        super().__init__(run_fold_name, params)\n",
    "\n",
    "    def train(self, tr_x, tr_y, va_x=None, va_y=None):\n",
    "        # scaling\n",
    "        validation = va_x is not None\n",
    "        exp = Experiment(\"MLP-Twitter\")\n",
    "        hd_callback = Hyperdash([\"val_loss\", \"loss\", \"val_accuracy\", \"accuracy\"], exp)\n",
    "\n",
    "        # パラメータ\n",
    "        nb_classes = 8\n",
    "        input_dropout = self.params['input_dropout']\n",
    "        hidden_layers = int(self.params['hidden_layers'])\n",
    "        hidden_units = int(self.params['hidden_units'])\n",
    "        hidden_activation = self.params['hidden_activation']\n",
    "        hidden_dropout = self.params['hidden_dropout']\n",
    "        batch_norm = self.params['batch_norm']\n",
    "        optimizer_type = self.params['optimizer']['type']\n",
    "        optimizer_lr = self.params['optimizer']['lr']\n",
    "        batch_size = int(self.params['batch_size'])\n",
    "        nb_epoch = int(self.params['nb_epoch'])\n",
    "\n",
    "        if issparse(tr_x):\n",
    "            scaler = StandardScaler(with_mean=False)\n",
    "        else:\n",
    "            scaler = StandardScaler()\n",
    "        scaler.fit(tr_x)\n",
    "        tr_x = scaler.transform(tr_x)\n",
    "        tr_y = np_utils.to_categorical(tr_y, num_classes=nb_classes)\n",
    "        if validation:\n",
    "            va_x =  scaler.transform(va_x)\n",
    "            va_y = np_utils.to_categorical(va_y, num_classes=nb_classes)\n",
    "\n",
    "        self.model = Sequential()\n",
    "        # input layer\n",
    "        self.model.add(Dropout(input_dropout, input_shape=(tr_x.shape[1],)))\n",
    "        # 中間層\n",
    "        for i in range(hidden_layers):\n",
    "            self.model.add(Dense(hidden_units))\n",
    "            if batch_norm == 'before_act':\n",
    "                self.model.add(BatchNormalization())\n",
    "            if hidden_activation == 'prelu':\n",
    "                self.model.add(PReLU())\n",
    "            elif hidden_activation == 'relu':\n",
    "                self.model.add(ReLU())\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            self.model.add(Dropout(hidden_dropout))\n",
    "\n",
    "        # 出力層\n",
    "        self.model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "        # オプティマイザ\n",
    "        if optimizer_type == 'sgd':\n",
    "            optimizer = SGD(lr=optimizer_lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        elif optimizer_type == 'adam':\n",
    "            optimizer = Adam(lr=optimizer_lr, beta_1=0.9, beta_2=0.999, decay=0.)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # 目的関数、評価指標などの設定\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        # エポック数、アーリーストッピング\n",
    "        # あまりepochを大きくすると、小さい学習率のときに終わらないことがあるので注意\n",
    "        patience = 10\n",
    "        # 学習の実行\n",
    "        if validation:\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=patience,\n",
    "                                            verbose=0, restore_best_weights=True)\n",
    "            history = self.model.fit(tr_x, tr_y, epochs=nb_epoch, batch_size=batch_size, verbose=0,\n",
    "                                validation_data=(va_x, va_y), callbacks=[early_stopping])\n",
    "        else:\n",
    "            history = self.model.fit(tr_x, tr_y, nb_epoch=nb_epoch, batch_size=batch_size, verbose=0)\n",
    "\n",
    "        self.scaler = scaler\n",
    "        exp.end()\n",
    "\n",
    "    def predict(self, te_x):\n",
    "        te_x = self.scaler.fit_transform(te_x)\n",
    "        y_pred = self.model.predict(te_x)\n",
    "        return y_pred\n",
    "\n",
    "    def score(self, te_x, te_y):\n",
    "        y_pred = self.predict(te_x)\n",
    "        return f1_score(np.identity(5)[te_y], np.identity(5)[np.argmax(y_pred, axis=1)], average='samples')\n",
    "\n",
    "    def save_model(self, feature):\n",
    "        model_path = os.path.join(f'./model/model/{feature}', f'{self.run_fold_name}.h5')\n",
    "        scaler_path = os.path.join(f'./model/model/{feature}', f'{self.run_fold_name}-scaler.pkl')\n",
    "        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "        self.model.save(model_path)\n",
    "        Util.dump(self.scaler, scaler_path)\n",
    "\n",
    "    def load_model(self, feature):\n",
    "        model_path = os.path.join(f'./model/model/{feature}', f'{self.run_fold_name}.h5')\n",
    "        scaler_path = os.path.join(f'./model/model/{feature}', f'{self.run_fold_name}-scaler.pkl')\n",
    "        self.model = load_model(model_path)\n",
    "        self.scaler = Util.load(scaler_path)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7892519,
     "status": "ok",
     "timestamp": 1576483118630,
     "user": {
      "displayName": "Takeshi Koshizuka",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCcRFcsMRDhF1wqEYEQ8zcMJDNe33BRGt73NxFcKQ=s64",
      "userId": "06262162256211987803"
     },
     "user_tz": -540
    },
    "id": "q_HjE_WiES86",
    "outputId": "a865c61d-5e9d-46f4-de47-9c916c0bacd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This run of MLP-Twitter ran for 0:01:01 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t05-47-07-346640.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 96.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.1, 'hidden_layers': 3.0, 'hidden_units': 32.0, 'input_dropout': 0.0, 'optimizer': {'lr': 0.00012954787338700032, 'type': 'sgd'}}, logloss: 1.3594\n",
      "This run of MLP-Twitter ran for 0:00:14 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t05-48-09-285218.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 96.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.65, 'hidden_layers': 3.0, 'hidden_units': 128.0, 'input_dropout': 0.0, 'optimizer': {'lr': 0.0039860624868401215, 'type': 'adam'}}, logloss: 1.4350\n",
      "This run of MLP-Twitter ran for 0:02:20 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t05-48-23-852244.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 64.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.1, 'hidden_layers': 3.0, 'hidden_units': 32.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 1.4194917837229644e-05, 'type': 'sgd'}}, logloss: 1.4852\n",
      "This run of MLP-Twitter ran for 0:00:42 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t05-50-44-552410.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 160.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.7000000000000001, 'hidden_layers': 4.0, 'hidden_units': 96.0, 'input_dropout': 0.05, 'optimizer': {'lr': 0.0032732047542285226, 'type': 'adam'}}, logloss: 1.4010\n",
      "This run of MLP-Twitter ran for 0:00:38 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t05-51-27-494943.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 160.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.4, 'hidden_layers': 3.0, 'hidden_units': 64.0, 'input_dropout': 0.05, 'optimizer': {'lr': 2.9260199758248904e-05, 'type': 'sgd'}}, logloss: 1.5330\n",
      "This run of MLP-Twitter ran for 0:00:23 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t05-52-06-394545.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 256.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.35000000000000003, 'hidden_layers': 3.0, 'hidden_units': 96.0, 'input_dropout': 0.0, 'optimizer': {'lr': 0.000502630887350691, 'type': 'sgd'}}, logloss: 1.3813\n",
      "This run of MLP-Twitter ran for 0:01:12 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t05-52-30-195408.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 32.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.1, 'hidden_layers': 4.0, 'hidden_units': 192.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 0.0035659208855445736, 'type': 'adam'}}, logloss: 1.2983\n",
      "This run of MLP-Twitter ran for 0:01:23 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t05-53-43-596691.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 64.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.65, 'hidden_layers': 3.0, 'hidden_units': 160.0, 'input_dropout': 0.0, 'optimizer': {'lr': 0.001518537492989313, 'type': 'adam'}}, logloss: 1.2721\n",
      "This run of MLP-Twitter ran for 0:01:17 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t05-55-08-129614.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 192.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.5, 'hidden_layers': 4.0, 'hidden_units': 64.0, 'input_dropout': 0.1, 'optimizer': {'lr': 0.0014323316805567505, 'type': 'adam'}}, logloss: 1.3118\n",
      "This run of MLP-Twitter ran for 0:00:26 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t05-56-26-868475.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 128.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.2, 'hidden_layers': 4.0, 'hidden_units': 192.0, 'input_dropout': 0.1, 'optimizer': {'lr': 0.00016609034712147192, 'type': 'adam'}}, logloss: 1.2763\n",
      "This run of MLP-Twitter ran for 0:00:38 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t05-56-54-195258.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 160.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.30000000000000004, 'hidden_layers': 3.0, 'hidden_units': 160.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 0.00039011565754210265, 'type': 'sgd'}}, logloss: 1.3540\n",
      "This run of MLP-Twitter ran for 0:00:12 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t05-57-33-518765.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 224.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.05, 'hidden_layers': 2.0, 'hidden_units': 256.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 0.007176308532523831, 'type': 'adam'}}, logloss: 1.3083\n",
      "This run of MLP-Twitter ran for 0:00:10 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t05-57-46-855934.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 224.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.45, 'hidden_layers': 2.0, 'hidden_units': 192.0, 'input_dropout': 0.1, 'optimizer': {'lr': 0.007115403684293818, 'type': 'adam'}}, logloss: 1.3306\n",
      "This run of MLP-Twitter ran for 0:01:04 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t05-57-58-048598.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 96.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.15000000000000002, 'hidden_layers': 3.0, 'hidden_units': 160.0, 'input_dropout': 0.1, 'optimizer': {'lr': 2.929857568977303e-05, 'type': 'sgd'}}, logloss: 1.4261\n",
      "This run of MLP-Twitter ran for 0:00:17 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t05-59-03-789771.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 192.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.15000000000000002, 'hidden_layers': 2.0, 'hidden_units': 192.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.0003638433086212619, 'type': 'adam'}}, logloss: 1.2890\n",
      "This run of MLP-Twitter ran for 0:00:12 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t05-59-22-418930.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 192.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.7000000000000001, 'hidden_layers': 4.0, 'hidden_units': 224.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 8.604010452581507e-05, 'type': 'sgd'}}, logloss: 1.5597\n",
      "This run of MLP-Twitter ran for 0:00:17 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t05-59-36-056043.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 224.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.2, 'hidden_layers': 3.0, 'hidden_units': 64.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 0.008503434616082951, 'type': 'sgd'}}, logloss: 1.2814\n",
      "This run of MLP-Twitter ran for 0:00:17 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t05-59-54-657780.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 224.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.6000000000000001, 'hidden_layers': 4.0, 'hidden_units': 128.0, 'input_dropout': 0.1, 'optimizer': {'lr': 3.473070971517235e-05, 'type': 'sgd'}}, logloss: 1.5606\n",
      "This run of MLP-Twitter ran for 0:00:27 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-00-13-407401.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 256.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.35000000000000003, 'hidden_layers': 2.0, 'hidden_units': 32.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 0.00034963556090050697, 'type': 'sgd'}}, logloss: 1.3852\n",
      "This run of MLP-Twitter ran for 0:02:04 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-00-42-281348.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 64.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.75, 'hidden_layers': 2.0, 'hidden_units': 192.0, 'input_dropout': 0.1, 'optimizer': {'lr': 0.00034921363115379575, 'type': 'sgd'}}, logloss: 1.4068\n",
      "This run of MLP-Twitter ran for 0:01:59 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-02-48-211263.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 128.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.55, 'hidden_layers': 4.0, 'hidden_units': 256.0, 'input_dropout': 0.05, 'optimizer': {'lr': 0.0002286135461195103, 'type': 'adam'}}, logloss: 1.2881\n",
      "This run of MLP-Twitter ran for 0:01:47 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-04-49-397237.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 32.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.25, 'hidden_layers': 4.0, 'hidden_units': 224.0, 'input_dropout': 0.05, 'optimizer': {'lr': 0.0007267051141689926, 'type': 'adam'}}, logloss: 1.2821\n",
      "This run of MLP-Twitter ran for 0:01:21 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-06-39-571066.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 128.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.8, 'hidden_layers': 4.0, 'hidden_units': 224.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.00010036165489211194, 'type': 'adam'}}, logloss: 1.5618\n",
      "This run of MLP-Twitter ran for 0:00:45 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-08-03-849497.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 64.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.5, 'hidden_layers': 2.0, 'hidden_units': 160.0, 'input_dropout': 0.05, 'optimizer': {'lr': 0.0011942364329249164, 'type': 'adam'}}, logloss: 1.2616\n",
      "This run of MLP-Twitter ran for 0:00:45 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-08-51-825209.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 64.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.55, 'hidden_layers': 2.0, 'hidden_units': 160.0, 'input_dropout': 0.0, 'optimizer': {'lr': 0.0011656492734118803, 'type': 'adam'}}, logloss: 1.2613\n",
      "This run of MLP-Twitter ran for 0:01:22 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-09-39-891660.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 32.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.5, 'hidden_layers': 2.0, 'hidden_units': 128.0, 'input_dropout': 0.05, 'optimizer': {'lr': 0.0007111040836369997, 'type': 'adam'}}, logloss: 1.2603\n",
      "This run of MLP-Twitter ran for 0:01:26 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-11-05-476101.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 32.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.55, 'hidden_layers': 2.0, 'hidden_units': 128.0, 'input_dropout': 0.0, 'optimizer': {'lr': 0.0006071542047496741, 'type': 'adam'}}, logloss: 1.2691\n",
      "This run of MLP-Twitter ran for 0:01:41 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-12-35-172978.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 32.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.45, 'hidden_layers': 2.0, 'hidden_units': 96.0, 'input_dropout': 0.05, 'optimizer': {'lr': 0.0004425173429423086, 'type': 'adam'}}, logloss: 1.2651\n",
      "This run of MLP-Twitter ran for 0:00:46 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-14-20-094934.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 96.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.6000000000000001, 'hidden_layers': 2.0, 'hidden_units': 128.0, 'input_dropout': 0.0, 'optimizer': {'lr': 0.002172328977285095, 'type': 'adam'}}, logloss: 1.2699\n",
      "This run of MLP-Twitter ran for 0:00:49 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-15-09-726421.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 64.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.4, 'hidden_layers': 2.0, 'hidden_units': 96.0, 'input_dropout': 0.0, 'optimizer': {'lr': 0.000839376178632181, 'type': 'adam'}}, logloss: 1.2654\n",
      "This run of MLP-Twitter ran for 0:01:07 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-16-02-415859.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 96.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.5, 'hidden_layers': 2.0, 'hidden_units': 128.0, 'input_dropout': 0.05, 'optimizer': {'lr': 0.0003373624130203423, 'type': 'adam'}}, logloss: 1.2627\n",
      "This run of MLP-Twitter ran for 0:01:48 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-17-13-471398.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 32.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.6000000000000001, 'hidden_layers': 2.0, 'hidden_units': 160.0, 'input_dropout': 0.0, 'optimizer': {'lr': 0.0017134339828433769, 'type': 'adam'}}, logloss: 1.2679\n",
      "This run of MLP-Twitter ran for 0:02:17 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-19-05-954442.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 64.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.8, 'hidden_layers': 2.0, 'hidden_units': 96.0, 'input_dropout': 0.0, 'optimizer': {'lr': 0.0005151942190526317, 'type': 'adam'}}, logloss: 1.2922\n",
      "This run of MLP-Twitter ran for 0:01:26 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-21-26-903947.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 96.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.7000000000000001, 'hidden_layers': 2.0, 'hidden_units': 128.0, 'input_dropout': 0.05, 'optimizer': {'lr': 0.0009788224801347562, 'type': 'adam'}}, logloss: 1.2667\n",
      "This run of MLP-Twitter ran for 0:01:41 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-22-57-489655.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 32.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.35000000000000003, 'hidden_layers': 3.0, 'hidden_units': 64.0, 'input_dropout': 0.0, 'optimizer': {'lr': 0.0022982480287114903, 'type': 'adam'}}, logloss: 1.2711\n",
      "This run of MLP-Twitter ran for 0:01:59 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-24-43-372242.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 64.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.65, 'hidden_layers': 2.0, 'hidden_units': 160.0, 'input_dropout': 0.05, 'optimizer': {'lr': 0.00020698890439483421, 'type': 'adam'}}, logloss: 1.2636\n",
      "This run of MLP-Twitter ran for 0:02:24 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-26-46-440426.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 96.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.55, 'hidden_layers': 3.0, 'hidden_units': 128.0, 'input_dropout': 0.0, 'optimizer': {'lr': 0.00012432390472416995, 'type': 'adam'}}, logloss: 1.2804\n",
      "This run of MLP-Twitter ran for 0:01:20 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-29-14-846906.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 128.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.45, 'hidden_layers': 2.0, 'hidden_units': 32.0, 'input_dropout': 0.05, 'optimizer': {'lr': 0.0002886702819198878, 'type': 'adam'}}, logloss: 1.2824\n",
      "This run of MLP-Twitter ran for 0:01:31 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-30-39-877022.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 32.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.30000000000000004, 'hidden_layers': 3.0, 'hidden_units': 96.0, 'input_dropout': 0.0, 'optimizer': {'lr': 0.0009843215457774933, 'type': 'adam'}}, logloss: 1.2681\n",
      "This run of MLP-Twitter ran for 0:01:09 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-32-16-250055.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 64.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.75, 'hidden_layers': 2.0, 'hidden_units': 224.0, 'input_dropout': 0.05, 'optimizer': {'lr': 0.0049416296848648475, 'type': 'adam'}}, logloss: 1.2736\n",
      "This run of MLP-Twitter ran for 0:00:39 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-33-30-468323.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 160.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.45, 'hidden_layers': 3.0, 'hidden_units': 160.0, 'input_dropout': 0.0, 'optimizer': {'lr': 0.009960082411360758, 'type': 'adam'}}, logloss: 1.2694\n",
      "This run of MLP-Twitter ran for 0:00:40 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-34-14-645203.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 96.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.4, 'hidden_layers': 2.0, 'hidden_units': 192.0, 'input_dropout': 0.05, 'optimizer': {'lr': 0.0025215083131680366, 'type': 'adam'}}, logloss: 1.2626\n",
      "This run of MLP-Twitter ran for 0:01:20 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-34-59-859450.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 64.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.65, 'hidden_layers': 2.0, 'hidden_units': 96.0, 'input_dropout': 0.0, 'optimizer': {'lr': 0.008723175187227192, 'type': 'sgd'}}, logloss: 1.2762\n",
      "This run of MLP-Twitter ran for 0:02:15 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-36-25-340286.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 32.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.30000000000000004, 'hidden_layers': 3.0, 'hidden_units': 64.0, 'input_dropout': 0.05, 'optimizer': {'lr': 0.0006390894407401336, 'type': 'adam'}}, logloss: 1.2754\n",
      "This run of MLP-Twitter ran for 0:00:25 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-38-45-534090.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 160.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.0, 'hidden_layers': 2.0, 'hidden_units': 160.0, 'input_dropout': 0.1, 'optimizer': {'lr': 0.004290778992485191, 'type': 'adam'}}, logloss: 1.3255\n",
      "This run of MLP-Twitter ran for 0:01:46 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-39-15-684978.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 128.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.5, 'hidden_layers': 3.0, 'hidden_units': 192.0, 'input_dropout': 0.1, 'optimizer': {'lr': 0.0018728463378221195, 'type': 'sgd'}}, logloss: 1.2747\n",
      "This run of MLP-Twitter ran for 0:00:54 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-41-07-712689.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 64.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.7000000000000001, 'hidden_layers': 2.0, 'hidden_units': 128.0, 'input_dropout': 0.0, 'optimizer': {'lr': 0.0032370381373734627, 'type': 'adam'}}, logloss: 1.3439\n",
      "This run of MLP-Twitter ran for 0:03:46 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-42-07-225928.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 32.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.75, 'hidden_layers': 2.0, 'hidden_units': 64.0, 'input_dropout': 0.05, 'optimizer': {'lr': 0.0012434246143314086, 'type': 'adam'}}, logloss: 1.3015\n",
      "This run of MLP-Twitter ran for 0:01:31 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-45-59-383760.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 96.0, 'hidden_activation': 'prelu', 'hidden_dropout': 0.65, 'hidden_layers': 3.0, 'hidden_units': 256.0, 'input_dropout': 0.0, 'optimizer': {'lr': 0.001745340197957116, 'type': 'sgd'}}, logloss: 1.3301\n",
      "This run of MLP-Twitter ran for 0:00:40 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-47-36-541621.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 192.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.55, 'hidden_layers': 2.0, 'hidden_units': 224.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.0017181920442568821, 'type': 'adam'}}, logloss: 1.2559\n",
      "This run of MLP-Twitter ran for 0:01:04 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-48-22-248645.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 192.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.25, 'hidden_layers': 3.0, 'hidden_units': 224.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.0024951861952728803, 'type': 'sgd'}}, logloss: 1.2571\n",
      "This run of MLP-Twitter ran for 0:00:38 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-49-32-410406.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 192.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.05, 'hidden_layers': 3.0, 'hidden_units': 256.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.002127813725150673, 'type': 'sgd'}}, logloss: 1.2913\n",
      "This run of MLP-Twitter ran for 0:00:50 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-50-16-286689.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 256.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.2, 'hidden_layers': 3.0, 'hidden_units': 224.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.004306990103435251, 'type': 'sgd'}}, logloss: 1.2690\n",
      "This run of MLP-Twitter ran for 0:01:31 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-51-12-383312.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 192.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.25, 'hidden_layers': 4.0, 'hidden_units': 256.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.001106566838194229, 'type': 'sgd'}}, logloss: 1.2813\n",
      "This run of MLP-Twitter ran for 0:00:53 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-52-50-084365.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 224.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.15000000000000002, 'hidden_layers': 4.0, 'hidden_units': 224.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.0001413262440697072, 'type': 'sgd'}}, logloss: 1.4194\n",
      "This run of MLP-Twitter ran for 0:00:52 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-53-49-343326.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 192.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.25, 'hidden_layers': 3.0, 'hidden_units': 256.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 0.004605428097675224, 'type': 'sgd'}}, logloss: 1.2601\n",
      "This run of MLP-Twitter ran for 0:01:21 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-54-47-819757.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 224.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.1, 'hidden_layers': 4.0, 'hidden_units': 192.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.0007812350529345997, 'type': 'sgd'}}, logloss: 1.2875\n",
      "This run of MLP-Twitter ran for 0:00:48 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-56-15-751042.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 256.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.35000000000000003, 'hidden_layers': 3.0, 'hidden_units': 224.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 1.0025478716398695e-05, 'type': 'sgd'}}, logloss: 1.5400\n",
      "This run of MLP-Twitter ran for 0:01:32 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-57-10-366081.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 160.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.2, 'hidden_layers': 3.0, 'hidden_units': 224.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 0.00017415092172278046, 'type': 'sgd'}}, logloss: 1.3495\n",
      "This run of MLP-Twitter ran for 0:01:31 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t06-58-49-514728.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 160.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.4, 'hidden_layers': 3.0, 'hidden_units': 192.0, 'input_dropout': 0.2, 'optimizer': {'lr': 6.172370433635959e-05, 'type': 'sgd'}}, logloss: 1.4505\n",
      "This run of MLP-Twitter ran for 0:01:04 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-00-27-684969.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 192.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.30000000000000004, 'hidden_layers': 4.0, 'hidden_units': 256.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 0.00588594727008395, 'type': 'sgd'}}, logloss: 1.2627\n",
      "This run of MLP-Twitter ran for 0:00:40 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-01-38-804401.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 224.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.05, 'hidden_layers': 3.0, 'hidden_units': 224.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.002969489570566614, 'type': 'sgd'}}, logloss: 1.2883\n",
      "This run of MLP-Twitter ran for 0:01:22 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-02-25-644095.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 256.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.15000000000000002, 'hidden_layers': 4.0, 'hidden_units': 256.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 0.0009280147261146686, 'type': 'sgd'}}, logloss: 1.2704\n",
      "This run of MLP-Twitter ran for 0:01:17 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-03-55-240904.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 224.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.0, 'hidden_layers': 3.0, 'hidden_units': 192.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 0.00019012051323524635, 'type': 'sgd'}}, logloss: 1.3316\n",
      "This run of MLP-Twitter ran for 0:01:26 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-05-19-296142.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 192.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.35000000000000003, 'hidden_layers': 3.0, 'hidden_units': 192.0, 'input_dropout': 0.2, 'optimizer': {'lr': 7.796592605344801e-05, 'type': 'sgd'}}, logloss: 1.4447\n",
      "This run of MLP-Twitter ran for 0:01:01 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-06-53-060720.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 192.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.25, 'hidden_layers': 3.0, 'hidden_units': 256.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 0.004431879950820056, 'type': 'sgd'}}, logloss: 1.2758\n",
      "This run of MLP-Twitter ran for 0:01:18 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-08-01-629921.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 192.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.25, 'hidden_layers': 3.0, 'hidden_units': 256.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.002885440473404958, 'type': 'sgd'}}, logloss: 1.2507\n",
      "This run of MLP-Twitter ran for 0:01:39 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-09-27-483201.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 160.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.30000000000000004, 'hidden_layers': 3.0, 'hidden_units': 224.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.0006690761688984881, 'type': 'sgd'}}, logloss: 1.2965\n",
      "This run of MLP-Twitter ran for 0:01:21 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-11-14-678967.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 224.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.1, 'hidden_layers': 3.0, 'hidden_units': 256.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.001254128288821773, 'type': 'sgd'}}, logloss: 1.2913\n",
      "This run of MLP-Twitter ran for 0:01:00 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-12-43-879719.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 192.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.2, 'hidden_layers': 3.0, 'hidden_units': 224.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.003050126091184784, 'type': 'sgd'}}, logloss: 1.2541\n",
      "This run of MLP-Twitter ran for 0:00:53 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-13-51-955800.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 160.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.2, 'hidden_layers': 3.0, 'hidden_units': 256.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.007007899098935499, 'type': 'sgd'}}, logloss: 1.2661\n",
      "This run of MLP-Twitter ran for 0:01:14 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-14-53-081178.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 256.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.15000000000000002, 'hidden_layers': 3.0, 'hidden_units': 224.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.0002398468199594042, 'type': 'sgd'}}, logloss: 1.3394\n",
      "This run of MLP-Twitter ran for 0:00:51 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-16-15-453987.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 192.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.2, 'hidden_layers': 3.0, 'hidden_units': 256.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.009950126331411143, 'type': 'sgd'}}, logloss: 1.2669\n",
      "This run of MLP-Twitter ran for 0:01:58 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-17-14-801144.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 128.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.35000000000000003, 'hidden_layers': 3.0, 'hidden_units': 192.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 0.0005201335403397959, 'type': 'sgd'}}, logloss: 1.3041\n",
      "This run of MLP-Twitter ran for 0:01:31 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-19-20-921641.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 224.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.6000000000000001, 'hidden_layers': 4.0, 'hidden_units': 224.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.003279566554620406, 'type': 'sgd'}}, logloss: 1.3978\n",
      "This run of MLP-Twitter ran for 0:01:43 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-21-00-853099.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 160.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.45, 'hidden_layers': 3.0, 'hidden_units': 256.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.0014156148212362823, 'type': 'sgd'}}, logloss: 1.2897\n",
      "This run of MLP-Twitter ran for 0:01:16 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-22-52-932856.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 256.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.1, 'hidden_layers': 3.0, 'hidden_units': 224.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 0.0005775897704965747, 'type': 'sgd'}}, logloss: 1.2993\n",
      "This run of MLP-Twitter ran for 0:00:47 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-24-17-865341.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 224.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.15000000000000002, 'hidden_layers': 3.0, 'hidden_units': 192.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.005989245942710865, 'type': 'adam'}}, logloss: 1.2804\n",
      "This run of MLP-Twitter ran for 0:01:41 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-25-13-774927.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 192.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.4, 'hidden_layers': 4.0, 'hidden_units': 160.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 1.9812026234150268e-05, 'type': 'sgd'}}, logloss: 1.5660\n",
      "This run of MLP-Twitter ran for 0:01:20 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-27-04-381714.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 160.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.55, 'hidden_layers': 3.0, 'hidden_units': 256.0, 'input_dropout': 0.1, 'optimizer': {'lr': 0.0001290690796829558, 'type': 'adam'}}, logloss: 1.2880\n",
      "This run of MLP-Twitter ran for 0:01:02 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-28-33-713978.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 128.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.45, 'hidden_layers': 3.0, 'hidden_units': 224.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.00959755683250645, 'type': 'adam'}}, logloss: 1.2755\n",
      "This run of MLP-Twitter ran for 0:01:35 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-29-45-414900.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 192.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.5, 'hidden_layers': 3.0, 'hidden_units': 224.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 0.005940771418641141, 'type': 'sgd'}}, logloss: 1.2594\n",
      "This run of MLP-Twitter ran for 0:00:58 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-31-30-240462.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 224.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.30000000000000004, 'hidden_layers': 4.0, 'hidden_units': 192.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.0029900263559397397, 'type': 'adam'}}, logloss: 1.2678\n",
      "This run of MLP-Twitter ran for 0:01:13 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-32-37-862298.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 160.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.05, 'hidden_layers': 2.0, 'hidden_units': 256.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.00042975573533423223, 'type': 'sgd'}}, logloss: 1.2902\n",
      "This run of MLP-Twitter ran for 0:00:53 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-34-00-750283.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 192.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.25, 'hidden_layers': 3.0, 'hidden_units': 160.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 0.007581939379347178, 'type': 'adam'}}, logloss: 1.2872\n",
      "This run of MLP-Twitter ran for 0:01:14 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-35-03-602002.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 256.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.55, 'hidden_layers': 2.0, 'hidden_units': 256.0, 'input_dropout': 0.1, 'optimizer': {'lr': 0.0002729125067799221, 'type': 'sgd'}}, logloss: 1.3758\n",
      "This run of MLP-Twitter ran for 0:01:35 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-36-27-647841.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 224.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.6000000000000001, 'hidden_layers': 3.0, 'hidden_units': 192.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.00016674661114961099, 'type': 'adam'}}, logloss: 1.3248\n",
      "This run of MLP-Twitter ran for 0:02:22 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-38-13-035535.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 128.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.35000000000000003, 'hidden_layers': 4.0, 'hidden_units': 224.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.0016037302713598639, 'type': 'sgd'}}, logloss: 1.2671\n",
      "This run of MLP-Twitter ran for 0:00:46 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-40-44-969762.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 160.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.05, 'hidden_layers': 2.0, 'hidden_units': 192.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 0.005325238189147347, 'type': 'adam'}}, logloss: 1.2979\n",
      "This run of MLP-Twitter ran for 0:00:56 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-41-41-052522.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 192.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.0, 'hidden_layers': 3.0, 'hidden_units': 224.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.0029251414234120706, 'type': 'sgd'}}, logloss: 1.3051\n",
      "This run of MLP-Twitter ran for 0:01:26 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-42-47-366157.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 224.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.15000000000000002, 'hidden_layers': 2.0, 'hidden_units': 160.0, 'input_dropout': 0.1, 'optimizer': {'lr': 0.001004307477166472, 'type': 'sgd'}}, logloss: 1.2849\n",
      "This run of MLP-Twitter ran for 0:01:41 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-44-23-987036.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 192.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.7000000000000001, 'hidden_layers': 3.0, 'hidden_units': 256.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 0.001616549975724292, 'type': 'adam'}}, logloss: 1.2686\n",
      "This run of MLP-Twitter ran for 0:00:54 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-46-15-963412.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 256.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.2, 'hidden_layers': 4.0, 'hidden_units': 224.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.00814073126269528, 'type': 'sgd'}}, logloss: 1.2704\n",
      "This run of MLP-Twitter ran for 0:01:11 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-47-20-386555.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 128.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.4, 'hidden_layers': 3.0, 'hidden_units': 192.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.0038406055918574203, 'type': 'adam'}}, logloss: 1.2573\n",
      "This run of MLP-Twitter ran for 0:01:37 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-48-42-269926.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 160.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.30000000000000004, 'hidden_layers': 2.0, 'hidden_units': 32.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 0.0043058892346319435, 'type': 'sgd'}}, logloss: 1.2720\n",
      "This run of MLP-Twitter ran for 0:01:06 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-50-30-416936.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 224.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.5, 'hidden_layers': 3.0, 'hidden_units': 256.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.0019044677521190285, 'type': 'adam'}}, logloss: 1.2587\n",
      "This run of MLP-Twitter ran for 0:01:41 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-51-47-505416.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 192.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.25, 'hidden_layers': 3.0, 'hidden_units': 160.0, 'input_dropout': 0.2, 'optimizer': {'lr': 5.491098280864142e-05, 'type': 'sgd'}}, logloss: 1.4305\n",
      "This run of MLP-Twitter ran for 0:00:51 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-53-39-960450.log\n",
      "\n",
      "params: {'batch_norm': 'no', 'batch_size': 160.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.65, 'hidden_layers': 2.0, 'hidden_units': 256.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 0.0007919933754862511, 'type': 'sgd'}}, logloss: 1.4507\n",
      "This run of MLP-Twitter ran for 0:01:50 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-54-41-957943.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 192.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.75, 'hidden_layers': 3.0, 'hidden_units': 224.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.00025122113500572246, 'type': 'adam'}}, logloss: 1.3662\n",
      "This run of MLP-Twitter ran for 0:01:43 and logs are available locally at: /root/.hyperdash/logs/mlp-twitter/mlp-twitter_2019-12-16t07-56-43-583794.log\n",
      "\n",
      "params: {'batch_norm': 'before_act', 'batch_size': 224.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.45, 'hidden_layers': 4.0, 'hidden_units': 192.0, 'input_dropout': 0.15000000000000002, 'optimizer': {'lr': 0.0022334964629559684, 'type': 'sgd'}}, logloss: 1.3276\n",
      "100%|██████████| 100/100 [2:11:30<00:00, 101.61s/it, best loss: 1.2506665286966405]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-16 07:58:38] - sdv - best params:{'batch_norm': 'before_act', 'batch_size': 192.0, 'hidden_activation': 'relu', 'hidden_dropout': 0.25, 'hidden_layers': 3.0, 'hidden_units': 256.0, 'input_dropout': 0.2, 'optimizer': {'lr': 0.002885440473404958, 'type': 'sgd'}}, score:1.2507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, STATUS_OK, Trials\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "def objective(params):\n",
    "    global param\n",
    "    param.update(params)\n",
    "    model = ModelMLP(\"MLP\", **param)\n",
    "    model.train(tr_x, tr_y, va_x, va_y)\n",
    "    va_pred = model.predict(va_x)\n",
    "    score = log_loss(va_y, va_pred)\n",
    "    print(f'params: {params}, logloss: {score:.4f}')\n",
    "    # 情報を記録しておく\n",
    "    history.append((params, score))\n",
    "    del model\n",
    "\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 基本となるパラメータ\n",
    "    param = {\n",
    "        'input_dropout': 0.0,\n",
    "        'hidden_layers': 3,\n",
    "        'hidden_units': 96,\n",
    "        'hidden_activation': 'relu',\n",
    "        'hidden_dropout': 0.2,\n",
    "        'batch_norm': 'before_act',\n",
    "        'optimizer': {'type': 'adam', 'lr': 0.001},\n",
    "        'batch_size': 64,\n",
    "        'nb_epoch': 100,\n",
    "    }\n",
    "    # 探索するパラメータの空間を指定する\n",
    "    param_space = {\n",
    "        'input_dropout': hp.quniform('input_dropout', 0, 0.2, 0.05),\n",
    "        'hidden_layers': hp.quniform('hidden_layers', 2, 4, 1),\n",
    "        'hidden_units': hp.quniform('hidden_units', 32, 256, 32),\n",
    "        'hidden_activation': hp.choice('hidden_activation', ['prelu', 'relu']),\n",
    "        'hidden_dropout': hp.quniform('hidden_dropout', 0, 0.8, 0.05),\n",
    "        'batch_norm': hp.choice('batch_norm', ['before_act', 'no']),\n",
    "        'optimizer': hp.choice('optimizer',\n",
    "                           [{'type': 'adam',\n",
    "                             'lr': hp.loguniform('adam_lr', np.log(0.0001), np.log(0.01))},\n",
    "                            {'type': 'sgd',\n",
    "                             'lr': hp.loguniform('sgd_lr', np.log(0.00001), np.log(0.01))}]),\n",
    "        'batch_size': hp.quniform('batch_size', 32, 256, 32)\n",
    "    }\n",
    "\n",
    "    #features = [\n",
    "    #    \"bow\",\"bow_nva\",\"bow_tf-idf\",\"term_2-gram\",\"term_3-gram\",\"word2vec_mean\",\"word2vec_pre_mean\",\n",
    "    #    \"word2vec_fine-tuning\", \"doc2vec\", \"scdv\", \"bert\"\n",
    "    #]\n",
    "    #features = [\n",
    "    #    \"tf-idf\",\"n-gram\",\"n-gram-tf-idf\"\n",
    "    #]\n",
    "    #features = [\n",
    "    #    \"word2vec_mean\", \"word2vec_max\", \"word2vec_concat\", \"word2vec_hier\", \"fasttext_mean\", \"fasttext_max\", \"fasttext_concat\", \"fasttext_hier\"\n",
    "    #]\n",
    "    features =  [\"sdv\"]\n",
    "\n",
    "    NAME = \"-\".join(features)\n",
    "\n",
    "    result = { }\n",
    "    for i, name in enumerate(features):\n",
    "        train_x = load_x_train(name)\n",
    "        train_y = load_y_train(name)\n",
    "        skf = StratifiedKFold(n_splits=6, shuffle=True, random_state=71)\n",
    "        tr_idx, va_idx = list(skf.split(train_x, train_y))[0]\n",
    "        tr_x, va_x = train_x[tr_idx], train_x[va_idx]\n",
    "        tr_y, va_y = train_y[tr_idx], train_y[va_idx]\n",
    "\n",
    "        # hyperoptによるパラメータ探索の実行\n",
    "        max_evals = 100\n",
    "        trials = Trials()\n",
    "        history = []\n",
    "        fmin(objective, param_space, algo=tpe.suggest, trials=trials, max_evals=max_evals)\n",
    "        history = sorted(history, key=lambda tpl: tpl[1])\n",
    "        best = history[0]\n",
    "        logger.info(f'{name} - best params:{best[0]}, score:{best[1]:.4f}')\n",
    "        \n",
    "        for key, value in best[0].items():\n",
    "            if key in result:\n",
    "                result[key].append(value)\n",
    "            else:\n",
    "                result[key] = [value]\n",
    "    res = pd.DataFrame.from_dict(\n",
    "        result,\n",
    "        orient='index',\n",
    "        columns=features\n",
    "    )\n",
    "    res.to_csv(f'./model/tuning/MLP-{NAME}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1VJXR2MBES7D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ffh4AsF7ES4j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TK35oqRFESzC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ndxkBPZNESwN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MLP_tuning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
